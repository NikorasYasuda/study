{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Entendendo o problema\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Em primeira instância:\n",
    "- Precisamos entender o contexto do problema que estamos analisando.\n",
    "- Qual tipo de problema é: classificação ou regressão?\n",
    "- Por que está sendo feito este estudo (qual é nosso objetivo)?\n",
    "- Qual medida de desempenho iremos utilizar para dizer se nosso modelo está realmente com uma alta acurácia e precisão?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Segunda instância:\n",
    "\n",
    "- Fazer a análise exploratória dos dados:\n",
    "    - Verificar se há valores faltantes e decidir como lidar com eles.\n",
    "    - Identificar as colunas numéricas e categóricas.\n",
    "    - Analisar a distribuição dos dados usando `df.describe()`. Se necessário, utilizar histogramas para uma análise visual mais detalhada com `df.hist()`.\n",
    "    - Verificar se os gráficos apresentam alguma anomalia, como cauda longa à direita, concentração em um ponto ou distribuições muito concentradas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nivel 1\n",
    "Após isso podemos fazer a parte mais importante para nosso modelo se tornar um modelo minimamente de nivel 1, separar o conjunto de dados em treinamento e teste.\n",
    "\n",
    "#### Alguns tipos de separação de dados:\n",
    "\n",
    "- Separação por valor de hash: É um metodo que consiste em seperar os dados para que não haja contaminação de conjunto no conjunto de teste ao fazer uma nova amostragem de dados caso necessarios.\n",
    "\n",
    "- Seperação estratificada: Imagine a seguinte questão em uma das colunas eu tenho o dado se a pessoa é menor de 18 e maior de 18, mas não necessariamente sei se isso vai causar algum tipo de impacto a analise que será feita, mas possivelmente sempre será uma boa opção segregar os dados de forma uniforme por exemplo no conjunto de treino terá a mesma quantidade proporcional que no conjunto de teste, para que assim não haja um enviasamento de dados.\n",
    "\n",
    "#### Após a separação é de extrema importância que só o conjunto de treinamento seja utilizado não devemos nunca tocar no conjunto de teste apenas na hora de testar o nosso modelo.\n",
    "\n",
    "- Caso uma contaminação ou um mal uso do conjunto de teste seja feito ele automaticamente se torna no conjunto de treino e temos que obter um novo conjunto de dados para que possamos testar posteriormente.\n",
    "\n",
    "\n",
    "#### Explorar e entender melhor os dados que temos em mão:\n",
    "\n",
    "- Visualizar e analisar cada coluna e suas correlações para podermos entende-las melhor.\n",
    "- Preparar os dados para o estudo se necessario.\n",
    "- Escolher um bom conjunto de modelos para o problema em questão, sendo ele de regressão ou classificação.\n",
    "- Treinar modelos e posteriormente fazer ajustes finos dos hiperparâmetros do modelo em si.\n",
    "\n",
    "\n",
    "##### Preparar os dados para o modelo:\n",
    "\n",
    "- Separar a variável dependente das variáveis indepentes para que possamos fazer as analises.\n",
    "- Criar novas colunas conforme os dados que nos temos para que isso possa nos auxiliar na analise mais aprofundada dos dados.\n",
    "- Transformar as variaveis categoricas em numericas (Seção - Codificando variáveis categóricas). *1\n",
    "- Tomar decisões necessarias sobre os possiveis valores faltantes do dataset.\n",
    "\n",
    "    - 1. Remover a coluna inteira de dados faltantes, ou\n",
    "        - 1.1 Esta coluna não é importante para nossa analise?\n",
    "\n",
    "        - 1.2 Esta coluna não aprensenta nenhuma correlação com outra que as duas juntas podem nos trazer informações importantes?\n",
    "        \n",
    "    - 2. Remover as linhas onde estão faltando dados, ou\n",
    "        - 2.1 Como isso pode nos afetar?\n",
    "        \n",
    "        - 2.2 Não são muitas linhas para simplesmente se eliminar?\n",
    "\n",
    "    - 3. Preencher os buracos\n",
    "        - 3.1 Preenche com zeros\n",
    "\n",
    "        - 3.2 Treinar um modelo de machine learning para prever os valores desta coluna e usar o modelo para preencher os espaços! Trata-se de uma forma mais sofisticada de interpolação.\n",
    "\n",
    "        - 3.3 Usar alguma estatística do dataset, como a mediana (IMPUTER)\n",
    "\n",
    "\n",
    "#### Manipulando os dados:\n",
    "\n",
    "*1 Se formos fazer um OneHotEnconder precisamos lembrar de sempre dropar uma das categoricas.\n",
    "\n",
    "\n",
    "Se Atentar a ColumnTransformer e Pipelines.\n",
    "\n",
    "#### Construindo modelos preditivos:\n",
    "\n",
    "- Na hora de aplicarmos o Fit em um modelo preditivo usamos model.fit(X,Y) sendo X as variveis independentes e Y a variavel dependente.\n",
    "- Quando começamos a validade se as familias de modelo que escolhemos é realmente boa para o problema em questão, podemos testar eles no conjunto de treinamente e vermos como os modelos escolhidos se comportam, para que assim possamos tomar a decisão de quais ou qual modelo seria o mais adequado para prosseguirmos treinando.\n",
    "- Devemos sempre tomar cuidado com overfitting ou underfitting.\n",
    "- Testamos o modelo com os erros absolutos e a medida de desempenho escolhida lá no começo.\n",
    "\n",
    "\n",
    "#### Nivel 2:\n",
    "\n",
    "- Validação cruzada é uma boa forma para que possamos testar os modelos escolhidos sem termos que tocar no conjunto de teste, passos para isso ser feito:\n",
    "    - Separamos novamente o conjunto de treinamento em 2, será o conjunto de treinamento e outro que será o conjunto de validação.\n",
    "    - Fazemos os teste multiplas vezes com conjuntos diferentes intercalando cada grupo de treino, validação com a função cross_val_score(model, adjusted_df, Y_df, error_measure, quantity_test)\n",
    "    - Teste paramétrico (tteste_ind), sendo p-value < 0.05 significante podendo nos dizer que a hipotese é valida.\n",
    "    - Teste não parametrico, teste de permutação onde fazemo uma analise utilizamos 2 amostras e reamostramos elas aleatoriamente para fazer essa comparação se o modelo é realmente melhor ou é simplesmente uma coincidência.\n",
    "\n",
    "    \n",
    "    ![Ilustração teste permutação](perm_orig.png \"Teste de permutação: configuração original\")\n",
    "\n",
    "    \n",
    "    ![Ilustração teste permutação](perm_perm.png \"Teste de permutação: configuração permutada\")\n",
    "\n",
    "#### Ajuste de Hiper Parâmetros:\n",
    "\n",
    "- Um modelo por si só é bom, mas podemos fazer ajustes finos neles que podem mudar completamente oque eles nos dizem geralmente simplesmente otimizamos o melhor modelo encotrado com o objetivo de maximizar seu desempenho esses ajustes vem com o multiplicador das features. Isso pode melhorar significantemente o nosso modelo existem formas de fazer isso por meio de calculos em alguns modelos porém em sua maioria temos que testar eles manualmente \"BRUTE FORCE\" gridSearch para testar um conjunto de hiperparamentros e ver quais desempenham melhor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "**Funções Relevantes**\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import percentileofscore\n",
    "import numpy as np\n",
    "def permutation_test(sample_1, sample_2, n_permutations=100):\n",
    "    diff_orig = np.mean(sample_1)- np.mean(sample_2)\n",
    "    n1 = len(sample_1)\n",
    "\n",
    "    # Junta as amostras em um grande array unificado.\n",
    "    pooled_samples = np.concatenate((sample_1, sample_2))\n",
    "\n",
    "    diffs = []\n",
    "    for i in range(n_permutations):\n",
    "        # Embaralha o grande array unificado e separa duas amostras fake.\n",
    "        np.random.shuffle(pooled_samples)\n",
    "        m1 = np.mean(pooled_samples[:n1])\n",
    "        m2 = np.mean(pooled_samples[n1:])\n",
    "        \n",
    "        # Guarda a diferença de médias.\n",
    "        diffs.append(m1 - m2)\n",
    "\n",
    "    # Determina o percentil em que se localizava a diferença de médias original.\n",
    "    q = percentileofscore(diffs, diff_orig) / 100.0\n",
    "    if q < 0.5:\n",
    "        pvalue = 2*q\n",
    "    else:\n",
    "        pvalue = 2*(1 - q)\n",
    "        \n",
    "    return pvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_scores(scores):\n",
    "    print(\"Scores (ordenados): [{}]\".format(\" \".join([\"{:.2f}\".format(x) for x in sorted(scores)])))\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Standard deviation:\", scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conceitual\n",
    "- Correlação de Spearman: A correlação de Spearman é uma correlação que analisa uma relação monotônica entre duas variaveis sendo Ordinais ou continuas, ela nós da se a correlação das duas variaveis são positivas (diretamente proporcional ou inversamente proporcional) e se elas são de alta intensidade ou de baixa intensidade, sendo de -1 a +1.\n",
    "- Correlação de Kendall: É uma correlação classificatoria, indo de -1 a 1 seguindo a mesma logica da correlação de Spearman.\n",
    "\n",
    "\n",
    "Ambas correlações são correlações alternativas para a correlação de Pearson quando a obrigações da mesma não são atendidas, elas sendo serem uma correlação entre uma variável dependente e cada uma das variáveis independentes e serem linearmente dependentes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
